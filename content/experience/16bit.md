---
title: "Technical Programmer"
description: "Care Computing Lab – IIT Delhi"
dateString: Oct 2023 - Dec 2023
draft: false
tags: ["EEG", "Deep Learning", "Neuroimaging", "Signal Processing", "Machine Learning"]
showToc: false
weight: 301
---

### Project Title: **EEG-Based Deep Learning for Auditory Processing**

**Lab:** Care Computing Lab, IIT Delhi  
**Supervisor:** Prof. Monika Aggarwal

As a **Technical Programmer** in the **Care Computing Lab** at IIT Delhi, I worked on an innovative project exploring the use of **electroencephalography (EEG)** to decode how the brain processes auditory stimuli, focusing on the relationship between **EEG signals** and **natural speech**. The project aimed to develop deep learning models that can help detect auditory attention and provide insights for **hearing loss diagnosis** and next-generation **smart hearing aids**.

### Project Overview:
Our project was part of the **Auditory-EEG Challenge**, where we worked on building models that can establish correlations between EEG signals and speech features. The challenge was particularly difficult due to the **low signal-to-noise ratio** of EEG data, requiring us to explore **non-linear methods** to enhance prediction accuracy. The project had two main tasks:

- **Task 1: Match-Mismatch**
  - Given two segments of speech and an EEG signal, we aimed to identify which segment of speech matched the brain’s response in the EEG.

- **Task 2: Regression**
  - The goal was to reconstruct the **speech envelope** from the EEG data, essentially decoding speech-related brain activity to create a direct mapping from the EEG to auditory stimuli.

### Key Responsibilities & Contributions:

1. **Data Preprocessing & Signal Representation**
   - Implemented advanced signal preprocessing techniques to improve the quality of EEG data, such as noise reduction and feature extraction, using techniques like **Fast Fourier Transform (FFT)** and **Wavelet Transform**.
   - Worked on creating **stimulus representations** that could better relate speech signals to the EEG data for model input.

2. **Deep Learning Model Development**
   - Developed and fine-tuned **deep learning models** using architectures like **Convolutional Neural Networks (CNNs)** and **Recurrent Neural Networks (RNNs)** to extract temporal and spatial features from EEG data.
   - Implemented **non-linear regression techniques** to enhance the model’s ability to map EEG data to speech signals, addressing the challenges posed by the noisy EEG data.

3. **Model Evaluation and Performance Enhancement**
   - Used metrics such as **Mean Squared Error (MSE)** and **Correlation Coefficients** to evaluate the model's ability to predict speech features from EEG.
   - Conducted rigorous **cross-validation** experiments to ensure that the models generalized well across different participants in the dataset.

4. **Collaborative Research & Insights**
   - Collaborated closely with researchers in the lab, as well as with faculty members, to refine our approach and interpret the results in the context of **auditory processing neuroscience**.
   - Contributed to research papers and presentations, discussing the potential impact of our findings on **smart hearing aids** and **diagnostic tools** for auditory disorders.

5. **Data Handling & Software Development**
   - Worked with large datasets (containing EEG data from 85 subjects), developing robust software solutions for data management and processing.
   - Contributed to the development of **data preprocessing pipelines** and **model deployment scripts**, which helped streamline the research process and make the analysis more efficient.

This project provided valuable experience in applying **deep learning** and **signal processing** techniques to real-world healthcare challenges, particularly in the fields of auditory neuroscience and smart medical devices.
---


