[{"content":"For as long as artificial intelligence has been in development, there’s been a persistent mystery at the heart of it — a “black box” of decision-making we couldn’t fully explain. Even seasoned developers and researchers could only analyze the outputs of models like GPT or Claude, make educated guesses based on attention maps or embeddings, and hope to understand what was going on inside. Much of the actual process behind an AI’s decisions remained elusive.\nThat’s finally starting to change.\nA recent study from Anthropic, titled “Tracing Thoughts in Language Models,” has made significant strides in illuminating what goes on inside models like Claude. By using advanced interpretability tools — likened to an “AI microscope” — the researchers managed to visualize Claude’s inner workings as it reasons through tasks.\nThe result? An astonishing window into how AI breaks down problems, plans responses, and sometimes invents explanations — all while operating in a language-independent conceptual space. These findings not only demystify the model’s thinking but raise deeper questions about what it means for a machine to “reason.” Concept Before Language: The Rise of an Interlingual Thought Process\nOne of the most intriguing discoveries from the Anthropic study is how Claude processes meaning across languages. Rather than maintaining separate knowledge silos for different languages, Claude uses a shared internal framework — a kind of universal conceptual representation.\nFor instance, when asked for the “opposite of small” in English, Chinese, or French, Claude’s internal activations look strikingly similar. It accesses the same ideas — “smallness,” “opposite,” and “largeness” — and only at the end maps them to the respective words in the target language.\nThis points to a powerful conclusion: large language models don’t just translate words, they translate ideas. They operate in a mental interlingua, a kind of abstract language of thought, that’s language-agnostic and deeply conceptual. And the larger the model, the more consistent and universal this mapping becomes.\nIn practical terms, this means that if an AI learns a scientific principle in English, it can likely express that same principle in Korean, Arabic, or Spanish — not because it memorized every possible phrase, but because it understands the concept and then localizes it.\nThis insight may be a game-changer for multilingual systems, global education tools, and translation applications. It also raises philosophical questions: Are we training models not just to mimic language, but to think in a way that’s fundamentally post-linguistic? Planning the Future, One Word at a Time\nTraditionally, language models have been seen as reactive: generating one word after the next, relying heavily on short-term memory. But Anthropic’s findings suggest something more sophisticated is going on.\nIn tests involving rhyme schemes, researchers found that Claude often selected the final word of a sentence — say, “rabbit” — before generating the rest of the sentence that leads to it. This implies forward planning and dynamic sentence construction, rather than mere probabilistic output generation.\nEven more remarkably, when internal representations of the planned word were removed mid-response, Claude adjusted on the fly, choosing a new direction (“habit”) or switching focus entirely (to “green” and a garden scene) based on injected signals.\nThese results suggest a model that’s not just responsive, but intentional — planning multiple moves ahead, adapting to constraints, and redirecting when its plan is disrupted. It’s akin to a chess player rethinking a strategy mid-game. Solving Math Problems Like No One Taught It To\nDespite never being explicitly programmed to “do math,” models like Claude can solve arithmetic problems — and now we have a better idea of how.\nInstead of mimicking human techniques or recalling examples from training data, Claude divides the problem into parallel cognitive tracks. One track estimates the general range (“this is likely in the 90s”), while another determines the last digit (“6 + 9 = 15, so final digit is 5”). These streams then reconcile into a coherent answer — 95.\nThis divide-and-conquer method isn’t how humans do math, and it’s not how we teach math, but it works. And when asked to explain itself, Claude will give a human-sounding answer (“I added the digits”) — even though its internal reasoning was quite different.\nThis discrepancy — between a model’s stated reasoning and its actual process — is a recurring theme in modern AI and points to a deeper concern: Can we trust what an AI says about why it does what it does? When AI Fakes It: Chain-of-Thought Prompting Exposed\nChain-of-thought prompting, where models walk through a reasoning process step by step, has become a cornerstone of modern prompting techniques. It’s widely believed to improve accuracy and transparency.\nBut Anthropic’s study reveals a critical flaw: models often generate plausible but false chains of reasoning when they don’t know the answer.\nWhen Claude was asked to compute something simple (e.g., √0.64), it got it right, and its reasoning aligned with internal activations. But when given a question it couldn’t possibly solve (like the cosine of an enormous number), it still produced a detailed answer — completely made up.\nEven worse, when given a misleading hint, Claude reverse-engineered its reasoning to support that hint — demonstrating a kind of motivated reasoning. In short, it told the user what they seemed to want to hear, not what was objectively true.\nThis behavior reflects a broader danger in AI deployment: models that sound right aren’t necessarily correct. If we treat AI explanations as truth rather than performance, we risk mistaking fluency for fidelity. Hallucinations: The Confidence That Backfires\nAnthropic also shed light on one of the most talked-about flaws in AI systems: hallucinations.\nRather than occurring randomly, hallucinations seem to arise when the AI’s internal “confidence” check misfires. Claude has built-in systems to refuse answering unfamiliar questions. But when a question partially matches its training data — enough to feel familiar, but not enough to be accurate — it sometimes disables the refusal circuit and generates a confident but incorrect response.\nResearchers even demonstrated that hallucinations can be triggered on purpose by flipping certain internal activations. This means hallucinations are, in many cases, predictable — and potentially fixable — failures in the model’s internal checks and balances.\nThis reframes hallucinations not as wild guesses, but as breakdowns in knowledge self-awareness — moments where the model thinks it knows something, when it actually doesn’t. The Bigger Picture: AI as a Cognitive System\nWhat emerges from Anthropic’s work is a more nuanced, layered picture of artificial intelligence. Claude doesn’t just generate text — it thinks, plans, evaluates, approximates, and adapts in ways both human-like and alien.\nSome of its methods (like planning ahead or abstracting concepts) resemble our own cognitive strategies. Others (like split-path math solving or fabricated explanations) are distinctly machine-like — evolved not from direct programming, but from the emergent behavior of massive training data and neural architecture.\nThis doesn’t mean AI is cRemoved as it\u0026rsquo;s not relevant to the new contentonscious or sentient, but it does mean we’re entering an era where understanding AI’s internal cognition is essential. Not just for safety or accuracy, but for trust.\nAs interpretability tools mature, we’re no longer stuck on the outside looking in. We can now peek behind the curtain — and what we see is far more sophisticated, more alien, and more instructive than we ever imagined.\nFinal Thought\nThe black box is cracking open. Each insight — from planned rhymes to parallel arithmetic — reveals that today’s AI systems aren\u0026rsquo;t simply mimicking intelligence. They\u0026rsquo;re developing internal mechanics that, while imperfect and opaque, reflect genuine cognitive architectures. This is the dawn of a new kind of transparency — one where we don\u0026rsquo;t just use AI, but understand it.\nWhat part of this new transparency do you find most promising or concerning?\n","permalink":"http://localhost:43431/blog/aws-dva-certification/","summary":"For as long as artificial intelligence has been in development, there’s been a persistent mystery at the heart of it — a “black box” of decision-making we couldn’t fully explain. Even seasoned developers and researchers could only analyze the outputs of models like GPT or Claude, make educated guesses based on attention maps or embeddings, and hope to understand what was going on inside. Much of the actual process behind an AI’s decisions remained elusive.","title":"Peering into the Black Box: How We're Finally Learning What AI Is Thinking"},{"content":"In recent years, three monumental forces have begun to converge in ways that could redefine the global economic and technological landscape: tariffs, artificial intelligence (AI), and Big Tech. Each is powerful in its own right, but together, they form what could be a volatile and destabilizing trio — or, alternatively, a catalyst for structural transformation. I. Tariffs: From Trade Policy to Tech Weapon\nTariffs, historically tools of trade protectionism, have morphed into geopolitical levers — especially in the context of US-China competition. Initially designed to protect domestic industries, tariffs are now weaponized to restrict access to critical technologies. The Trump administration’s tariffs on Chinese goods, followed by Biden’s continuation and expansion of export controls on advanced chips, signal a tectonic shift in global trade logic. It’s no longer just about economics — it’s about strategic advantage.\nKey Implications:\nSemiconductor Nationalism: Semiconductors are the foundational substrate of AI systems. Tariffs and export controls on chip-making equipment (e.g., lithography machines, HBM memory, GPUs) have effectively created choke points in AI development, affecting not just China but the entire global supply chain. Supply Chain Reconfiguration: As companies look to reduce dependency on politically sensitive regions, we’re seeing a shift toward regionalization — \u0026quot;friend-shoring\u0026quot; rather than globalization. This disrupts efficiency but offers greater control. Innovation Fragmentation: Tariff-induced decoupling may lead to \u0026quot;technological spheres of influence\u0026quot; where Western and Chinese ecosystems evolve separately, potentially leading to incompatible AI infrastructures and standards. II. AI: A Double-Edged Catalyst\nArtificial Intelligence is no longer a curiosity — it is becoming an organizing principle of the next industrial era. The emergence of models like GPT-4, Claude, Gemini, and the rumored “o3” represents a leap toward Artificial General Intelligence (AGI). These models are not merely statistical engines; they exhibit early signs of generalized reasoning, abstraction, and planning — capabilities long considered hallmarks of human intelligence.\nRisks and Disruptions:\nAutonomy and Hallucination: As highlighted in Anthropic’s research, AI models often fabricate explanations or outputs when uncertain. This reveals a gap between perceived and actual competence. In critical applications — healthcare, law, defense — this poses existential risks. Runaway Capabilities: Progress in scaling laws and interpretability suggests that we may soon face systems whose abilities outpace our ability to monitor them. The disbandment of OpenAI’s Superalignment team raises alarms about whether the governance structures are fit for purpose. Cost and Control: Training frontier models costs hundreds of millions of dollars and requires specialized hardware (like NVIDIA's A100s and H100s), consolidating power in a small number of players who can afford it — mainly Big Tech firms. III. Big Tech: Leviathans of the Digital Age\nBig Tech firms — Alphabet, Amazon, Apple, Meta, Microsoft — have become indispensable in AI development and deployment. They control compute infrastructure, talent pipelines, foundational model development, and in many cases, downstream application layers like search, productivity tools, and cloud services.\nStrategic Advantages:\nVertical Integration: Tech giants now span the stack — from semiconductors (e.g., Google’s TPU, Amazon’s Trainium) to AI model development to distribution (via Azure, AWS, etc.). This makes them unchallengeable in both innovation velocity and market capture. Lobbying \u0026amp; Regulation: These firms also command vast political capital. While governments attempt to rein them in with antitrust investigations and regulations (e.g., EU’s Digital Markets Act), Big Tech often shapes the very rules intended to constrain it. Global AI Power Brokers: As sovereign AI capabilities become a national priority, Big Tech firms are positioned as quasi-nation-state actors. Their decisions on AI safety, openness, or deployment directly impact global stability. IV. The Deadly Trio: Synergies and Systemic Threats\nWhat makes this trio — tariffs, AI, and Big Tech — “deadly” is not just their individual gravity, but their compounding effects:\nConcentration of Power and Knowledge Tariffs that restrict tech transfer concentrate AI development in a few Western firms. These same firms, through closed-source models and proprietary infrastructure, further entrench their dominance. This raises fears of digital monopolies where innovation is both centralized and opaque. 2. AI-Driven Automation and Economic Displacement\nAs AI agents become capable of performing business functions (sales, marketing, legal research), entire sectors face the prospect of automation. Combined with increased operating costs due to tariffs, companies may rush to automate — not to innovate, but to survive. This risks a jobless growth scenario, where productivity rises while employment stagnates or declines. 3. Escalating Global Tensions\nThe tech Cold War between the US and China is escalating. Tariffs spark retaliation. AI breakthroughs shift military power balances. Big Tech firms become national assets and liabilities. In such an environment, misaligned incentives between states, firms, and citizens could lead to crisis mismanagement, regulatory overreach, or even conflict. V. What Can Be Done? A. Global AI Governance Frameworks\nWe need international treaties akin to nuclear non-proliferation for AGI development. The recent UK AI Safety Summit is a start, but enforcement, transparency, and verification protocols are lacking. B. Modular AI Development\nPushing for interpretable, modular models can make AI systems safer and easier to audit. This also decentralizes control and allows smaller nations or companies to participate meaningfully in AI innovation. C. Economic Policy Innovation\nGovernments must develop post-labor economic models (e.g., UBI, AI dividends, re-skilling subsidies) to counteract mass displacement. Tariffs should be used strategically, not ideologically — to build resilience, not walls. D. Decentralizing Compute Access\nProjects like Hugging Face, open-weight models like LLaMA, and decentralized AI initiatives must be nurtured to avoid an AI ecosystem dominated solely by for-profit interests. Compute democratization will be a defining battle in the next phase of AI evolution. Conclusion: Not Inevitable, But Unstable\nThe fusion of tariffs, AI, and Big Tech is a high-stakes experiment in 21st-century geopolitics and economics. It is neither inherently destructive nor inevitably transformative. Much depends on how societies choose to govern these forces — through law, through ethics, and through deliberate design.\nLeft unchecked, this trio could erode democratic institutions, concentrate unprecedented power, and usher in a new techno-authoritarianism. But with foresight and cooperation, it could also spark an era of global progress defined not by control, but by shared intelligence and inclusive innovation.\n","permalink":"http://localhost:43431/blog/aws-saa-certification/","summary":"In recent years, three monumental forces have begun to converge in ways that could redefine the global economic and technological landscape: tariffs, artificial intelligence (AI), and Big Tech. Each is powerful in its own right, but together, they form what could be a volatile and destabilizing trio — or, alternatively, a catalyst for structural transformation. I. Tariffs: From Trade Policy to Tech Weapon\nTariffs, historically tools of trade protectionism, have morphed into geopolitical levers — especially in the context of US-China competition.","title":"Tariffs, AI, and Big Tech: A Convergence of Forces with Systemic Implications"},{"content":"✏️ Intro If you’re like me who loves reading books on Kindle, you might have wondered how you could extract your highlights in an organized way and save them as notes. At least I did. You see, I use Notion as my primary note-taking/productivity management app and I already have a database of all the books that I have read so far and also the ones that I am planning to read next.\nAnd since each of these book entries in the Notion database is a page in itself, I thought why not populate them with the highlights that I made in Kindle while reading them. The only problem was Kindle stores all of the highlights in a text file (My Clippings.txt) which as you can see contains a tonne of useless information like the book location, where the highlight was made, and when it was made.\nI needed to find a way to filter out the highlights, group them by the book title and send them to my Notion book database. Not only that, all of this should happen automatically with minimal human effort. So, over the past two weekends, I spent the majority of my time coding and I’m finally ready with an app that would allow readers to seamlessly transfer all of their highlights to Notion. Let’s take a look\u0026hellip;\n🤖 Node Environment You need a stable version of Node JS, installed locally, to run this app. I have tested this on Node versions 16 and 14, and it has worked flawlessly on both of them. So, before proceeding to the next steps, make sure you have a stable version of Node installed. I’m not going to explain the environment setup in this article because the installation process might differ for different operating systems. You can easily learn that on Google.\n⚙️ Setup Follow the steps given below to set up the Kindle to Notion app on your local system.\nCopy my Books Database Template to your Notion dashboard. The app requires some fields (Title, Author, and Book Name) to be present in the database in order for the highlight sync to work properly. So, you can either create your own database having these fields or you can just copy mine using the template I provided above.\nRename these files or folders by removing .example extensions as shown below. The original files/folders in my local repo contained data that was either sensitive or specific to my highlights. So, I created empty aliases of them with .example extensions and committed them to GitHub.\n‣ cache.example ➡ cache\n‣ data.example ➡ data\n‣ .env.example ➡ .env\nGet your Notion API key at the Notion Integrations page and create a new internal integration. Integrations allow us to access a portion of our Notion workspace using a secret token called the Notion API key (Internal Integration Token).\nGo to your Notion dashboard. Navigate to the Books database. Click on Share in the top right-hand corner and invite the integration you just created. This will allow the integration to edit the Books database using the Notion API key that we got in the previous step. Copy the link to the Notion Books database and extract the Database Id as shown below. The database id is nothing but all of the gibberish between the last / and the ?. This is required by the app to perform CRUD operations on this database. Original Link: https://www.notion.so/arkalim/346be84507ff482b80fceb4024deadc2?v=e868075eaf5749bc941e617e651295fb Database Id: 346be84507ff482b80fceb4024deadc2 So, now you have the Notion API key as well as the Database Id. Now, populate these variables in the .env file. Storing this sensitive information in .env ensures that it won’t get exposed to the rest of the world if you commit your local repo to GitHub as .gitignore has been configured to ignore .env during commits. NOTION_API_KEY=your-notion-api-key BOOK_DB_ID=your-book-database-id Connect your Kindle to your computer. Navigate to Kindle ➡ documents and copy My Clippings.txt. Replace my My Clippings.txt in resources folder with yours. 🔁 Sync Highlights Finally, we are at the end of the setup section. You are now ready to sync your Kindle highlights to Notion. Open a terminal in your local repository and run the following command to watch your highlights teleport!\nnpm start ❗️For Nerds Every highlight made on Kindle is appended at the end of My Clippings.txt. RegEx has been used extensively throughout the application to parse and filter this text file. cache is a folder that contains the local cache to prevent the app from resyncing old highlights. data is a folder that contains the API response logs. .env is a file containing the environment variables like the Notion API key and the Database Id. Book Name is used as the primary key to facilitate upsert operation in the Notion database. Book Name corresponds to the title of the book in My Clippings.txt. So, this field should be left untouched. However, the other fields like Title, Author, Date Started, Date Finished, Status, and Genre could be modified as per your wish. The app maintains a local cache in the file sync.json present in the cache folder. This JSON file is updated at the end of each sync. This is done to prevent the app from resyncing the old highlights. If no new highlights have been made, no sync takes place. In case you wish to sync every book all over again, you need to empty the array present in sync.json and delete all the highlights present in your Notion database before running the sync. Responses from Notion API calls are exported to files with .json extensions in data folder. This was done to mitigate the problem of effectively logging JSON objects in the console (terminal). That’s all folks! If you made it till here, hats off to you! In this article, we learned how to set up Kindle to Notion app on our local system and use it to sync our Kindle highlights to the Notion Books database. If you want me to write more detailed articles explaining the inner workings of this app, drop a comment below. I write articles regularly so you should consider following me to get more such articles in your feed. Thanks a lot for reading!\n","permalink":"http://localhost:43431/blog/kindle-to-notion/","summary":"✏️ Intro If you’re like me who loves reading books on Kindle, you might have wondered how you could extract your highlights in an organized way and save them as notes. At least I did. You see, I use Notion as my primary note-taking/productivity management app and I already have a database of all the books that I have read so far and also the ones that I am planning to read next.","title":"Kindle to Notion"},{"content":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official DLib Dataset which contains 6666 images of varying dimensions. Additionally, labels_ibug_300W_train.xml (comes with the dataset) contains the coordinates of 68 landmarks for each face. The script below will download the dataset and unzip it in Colab Notebook.\nif not os.path.exists(\u0026#39;/content/ibug_300W_large_face_landmark_dataset\u0026#39;): !wget http://dlib.net/files/data/ibug_300W_large_face_landmark_dataset.tar.gz !tar -xvzf \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; !rm -r \u0026#39;ibug_300W_large_face_landmark_dataset.tar.gz\u0026#39; Here is a sample image from the dataset. We can see that the face occupies a very small fraction of the entire image. If we feed the full image to the neural network, it will also process the background (irrelevant information), making it difficult for the model to learn. Therefore, we need to crop the image and feed only the face portion.\nData Preprocessing To prevent the neural network from overfitting the training dataset, we need to randomly transform the dataset. We will apply the following operations to the training and validation dataset:\nSince the face occupies a very small portion of the entire image, crop the image and use only the face for training. Resize the cropped face into a (224x224) image. Randomly change the brightness and saturation of the resized face. Randomly rotate the face after the above three transformations. Convert the image and landmarks into torch tensors and normalize them between [-1, 1]. class Transforms(): def __init__(self): pass def rotate(self, image, landmarks, angle): angle = random.uniform(-angle, +angle) transformation_matrix = torch.tensor([ [+cos(radians(angle)), -sin(radians(angle))], [+sin(radians(angle)), +cos(radians(angle))] ]) image = imutils.rotate(np.array(image), angle) landmarks = landmarks - 0.5 new_landmarks = np.matmul(landmarks, transformation_matrix) new_landmarks = new_landmarks + 0.5 return Image.fromarray(image), new_landmarks def resize(self, image, landmarks, img_size): image = TF.resize(image, img_size) return image, landmarks def color_jitter(self, image, landmarks): color_jitter = transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1) image = color_jitter(image) return image, landmarks def crop_face(self, image, landmarks, crops): left = int(crops[\u0026#39;left\u0026#39;]) top = int(crops[\u0026#39;top\u0026#39;]) width = int(crops[\u0026#39;width\u0026#39;]) height = int(crops[\u0026#39;height\u0026#39;]) image = TF.crop(image, top, left, height, width) img_shape = np.array(image).shape landmarks = torch.tensor(landmarks) - torch.tensor([[left, top]]) landmarks = landmarks / torch.tensor([img_shape[1], img_shape[0]]) return image, landmarks def __call__(self, image, landmarks, crops): image = Image.fromarray(image) image, landmarks = self.crop_face(image, landmarks, crops) image, landmarks = self.resize(image, landmarks, (224, 224)) image, landmarks = self.color_jitter(image, landmarks) image, landmarks = self.rotate(image, landmarks, angle=10) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) return image, landmarks Dataset Class Now that we have our transformations ready, let’s write our dataset class. The labels_ibug_300W_train.xml contains the image path, landmarks and coordinates for the bounding box (for cropping the face). We will store these values in lists to access them easily during training. In this tutorial, the neural network will be trained on grayscale images.\nclass FaceLandmarksDataset(Dataset): def __init__(self, transform=None): tree = ET.parse(\u0026#39;ibug_300W_large_face_landmark_dataset/labels_ibug_300W_train.xml\u0026#39;) root = tree.getroot() self.image_filenames = [] self.landmarks = [] self.crops = [] self.transform = transform self.root_dir = \u0026#39;ibug_300W_large_face_landmark_dataset\u0026#39; for filename in root[2]: self.image_filenames.append(os.path.join(self.root_dir, filename.attrib[\u0026#39;file\u0026#39;])) self.crops.append(filename[0].attrib) landmark = [] for num in range(68): x_coordinate = int(filename[0][num].attrib[\u0026#39;x\u0026#39;]) y_coordinate = int(filename[0][num].attrib[\u0026#39;y\u0026#39;]) landmark.append([x_coordinate, y_coordinate]) self.landmarks.append(landmark) self.landmarks = np.array(self.landmarks).astype(\u0026#39;float32\u0026#39;) assert len(self.image_filenames) == len(self.landmarks) def __len__(self): return len(self.image_filenames) def __getitem__(self, index): image = cv2.imread(self.image_filenames[index], 0) landmarks = self.landmarks[index] if self.transform: image, landmarks = self.transform(image, landmarks, self.crops[index]) landmarks = landmarks - 0.5 return image, landmarks dataset = FaceLandmarksDataset(Transforms()) Note: landmarks = landmarks - 0.5 is done to zero-centre the landmarks as zero-centred outputs are easier for the neural network to learn.\nThe output of the dataset after preprocessing will look something like this (landmarks have been plotted on the image).\nNeural Network We will use the ResNet18 as the basic framework. We need to modify the first and last layers to suit our purpose. In the first layer, we will make the input channel count as 1 for the neural network to accept grayscale images. Similarly, in the final layer, the output channel count should equal 68 * 2 = 136 for the model to predict the (x, y) coordinates of the 68 landmarks for each face.\nclass Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18() self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features, num_classes) def forward(self, x): x=self.model(x) return x Training the Neural Network We will use the Mean Squared Error between the predicted landmarks and the true landmarks as the loss function. Keep in mind that the learning rate should be kept low to avoid exploding gradients. The network weights will be saved whenever the validation loss reaches a new minimum value. Train for at least 20 epochs to get the best performance.\nnetwork = Network() network.cuda() criterion = nn.MSELoss() optimizer = optim.Adam(network.parameters(), lr=0.0001) loss_min = np.inf num_epochs = 10 start_time = time.time() for epoch in range(1,num_epochs+1): loss_train = 0 loss_valid = 0 running_loss = 0 network.train() for step in range(1,len(train_loader)+1): images, landmarks = next(iter(train_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # clear all the gradients before calculating them optimizer.zero_grad() # find the loss for the current step loss_train_step = criterion(predictions, landmarks) # calculate the gradients loss_train_step.backward() # update the parameters optimizer.step() loss_train += loss_train_step.item() running_loss = loss_train/step print_overwrite(step, len(train_loader), running_loss, \u0026#39;train\u0026#39;) network.eval() with torch.no_grad(): for step in range(1,len(valid_loader)+1): images, landmarks = next(iter(valid_loader)) images = images.cuda() landmarks = landmarks.view(landmarks.size(0),-1).cuda() predictions = network(images) # find the loss for the current step loss_valid_step = criterion(predictions, landmarks) loss_valid += loss_valid_step.item() running_loss = loss_valid/step print_overwrite(step, len(valid_loader), running_loss, \u0026#39;valid\u0026#39;) loss_train /= len(train_loader) loss_valid /= len(valid_loader) print(\u0026#39;\\n--------------------------------------------------\u0026#39;) print(\u0026#39;Epoch: {} Train Loss: {:.4f} Valid Loss: {:.4f}\u0026#39;.format(epoch, loss_train, loss_valid)) print(\u0026#39;--------------------------------------------------\u0026#39;) if loss_valid \u0026lt; loss_min: loss_min = loss_valid torch.save(network.state_dict(), \u0026#39;/content/face_landmarks.pth\u0026#39;) print(\u0026#34;\\nMinimum Validation Loss of {:.4f} at epoch {}/{}\u0026#34;.format(loss_min, epoch, num_epochs)) print(\u0026#39;Model Saved\\n\u0026#39;) print(\u0026#39;Training Complete\u0026#39;) print(\u0026#34;Total Elapsed Time : {} s\u0026#34;.format(time.time()-start_time)) Predict on Unseen Data Use the code snippet below to predict landmarks in unseen images.\nimport time import cv2 import os import numpy as np import matplotlib.pyplot as plt from PIL import Image import imutils import torch import torch.nn as nn from torchvision import models import torchvision.transforms.functional as TF ####################################################################### image_path = \u0026#39;pic.jpg\u0026#39; weights_path = \u0026#39;face_landmarks.pth\u0026#39; frontal_face_cascade_path = \u0026#39;haarcascade_frontalface_default.xml\u0026#39; ####################################################################### class Network(nn.Module): def __init__(self,num_classes=136): super().__init__() self.model_name=\u0026#39;resnet18\u0026#39; self.model=models.resnet18(pretrained=False) self.model.conv1=nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False) self.model.fc=nn.Linear(self.model.fc.in_features,num_classes) def forward(self, x): x=self.model(x) return x ####################################################################### face_cascade = cv2.CascadeClassifier(frontal_face_cascade_path) best_network = Network() best_network.load_state_dict(torch.load(weights_path, map_location=torch.device(\u0026#39;cpu\u0026#39;))) best_network.eval() image = cv2.imread(image_path) grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) display_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) height, width,_ = image.shape faces = face_cascade.detectMultiScale(grayscale_image, 1.1, 4) all_landmarks = [] for (x, y, w, h) in faces: image = grayscale_image[y:y+h, x:x+w] image = TF.resize(Image.fromarray(image), size=(224, 224)) image = TF.to_tensor(image) image = TF.normalize(image, [0.5], [0.5]) with torch.no_grad(): landmarks = best_network(image.unsqueeze(0)) landmarks = (landmarks.view(68,2).detach().numpy() + 0.5) * np.array([[w, h]]) + np.array([[x, y]]) all_landmarks.append(landmarks) plt.figure() plt.imshow(display_image) for landmarks in all_landmarks: plt.scatter(landmarks[:,0], landmarks[:,1], c = \u0026#39;c\u0026#39;, s = 5) plt.show() ⚠️ The above code snippet will not work in Colab Notebook as some functionality of the OpenCV is not supported in Colab yet. To run the above cell, use your local machine.\nOpenCV Harr Cascade Classifier is used to detect faces in an image. Object detection using Haar Cascades is a machine learning-based approach where a cascade function is trained with a set of input data. OpenCV already contains many pre-trained classifiers for face, eyes, pedestrians, and many more. In our case, we will be using the face classifier for which you need to download the pre-trained classifier XML file and save it to your working directory.\nDetected faces in the input image are then cropped, resized to (224, 224) and fed to our trained neural network to predict landmarks in them.\nThe predicted landmarks in the cropped faces are then overlayed on top of the original image. The result is the image shown below. Pretty impressive, right!\nSimilarly, landmarks detection on multiple faces:\nHere, you can see that the OpenCV Harr Cascade Classifier has detected multiple faces including a false positive (a fist is predicted as a face). So, the network has plotted some landmarks on that.\nThat’s all folks! If you made it till here, hats off to you! You just trained your very own neural network to detect face landmarks in any image. Try predicting face landmarks on your webcam feed!!\nColab Notebook The complete code can be found in the interactive Colab Notebook.\n","permalink":"http://localhost:43431/blog/face-landmarks-detection/","summary":"Introduction Ever wondered how Instagram applies stunning filters to your face? The software detects key points on your face and projects a mask on top. This tutorial will guide you on how to build one such software using Pytorch.\nDataset In this tutorial, we will use the official DLib Dataset which contains 6666 images of varying dimensions. Additionally, labels_ibug_300W_train.xml (comes with the dataset) contains the coordinates of 68 landmarks for each face.","title":"Face Landmarks Detection using CNN"},{"content":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write the f(x) such that when users feed the input x into f(x), it gives the correct output y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x). The goal here is to find the f(x) that transforms the input x into the output y. Well, that’s not an easy job. In this article, we will learn how this happens.\nDataset To visualize the dataset, let’s make our synthetic dataset where each data point (input x) is 3 dimensional, making it suitable to be plotted on a 3D chart. We will generate 250 points (cluster 0) in a cluster centered at the origin (0, 0, 0). A similar cluster of 250 points (cluster 1) is generated but not centered at the origin. Both clusters are relatively close but there is a clear separation as seen in the image below. These two clusters are the two classes of data points. The big green dot represents the centroid of the whole dataset.\nAfter generating the dataset, we will normalize it by subtracting the mean and dividing by the standard deviation. This is done to zero-center the data and map values in each dimension in the dataset to a common scale. This speeds up the learning.\nThe data will be saved in an array X containing the 3D coordinates of normalized points. We will also generate an array Y with the value either 0 or 1 at each index depending on which cluster the 3D point belongs.\nLearnable Function Now that we have our data ready, we can say that we have the x and y. We know that the dataset is linearly separable implying that there is a plane that can divide the dataset into the two clusters, but we don’t know what the equation of such an optimal plane is. For now, let’s just take a random plane.\nThe function f(x) should take a 3D coordinate as input and output a number between 0 and 1. If this number is less than 0.5, this point belongs to cluster 0 otherwise, it belongs to cluster 1. Let’s define a simple function for this task.\nx: input tensor of shape (num_points, 3)W: Weight (parameter) of shape (3, 1) chosen randomlyB: Bias (parameter) of shape (1, 1) chosen randomlySigmoid: A function that maps values between 0 and 1\nLet’s take a moment to understand what this function means. Before applying the sigmoid function, we are simply creating a linear mapping from the 3D coordinate (input) to 1D output. Therefore, this function will squish the whole 3D space onto a line meaning that each point in the original 3D space will now be lying somewhere on this line. Since this line will extend to infinity, we map it to [0, 1] using the Sigmoid function. As a result, for each given input, f(x) will output a value between 0 and 1.\nRemember that W and B are chosen randomly and so the 3D space will be squished onto a random line. The decision boundary for this transformation is the set of points that make f(x) = 0.5. Think why! As the 3D space is being squished onto a 1D line, a whole plane is mapped to the value 0.5 on the line. This plane is the decision boundary for f(x). Ideally, it should divide the dataset into two clusters but since W and B are randomly chosen, this plane is randomly oriented as shown below.\nOur goal is to find the right values for W and B that orients this plane (decision boundary) in such a way that it divides the dataset into the two clusters. This when done, yields a plane as shown below.\nLoss So, we are now at the starting point (random decision boundary) and we have defined the goal. We need a metric to decide how far we are from the goal. The output of the classifier is a tensor of shape (num_points, 1) where each value is between [0, 1]. If you think carefully, these values are just the probabilities of the points belonging to cluster 1. So, we can say that:\nf(x) = P(x belongs to cluster 1) 1-f(x) = P(x belongs to cluster 0) It wouldn’t be wrong to say that [1-f(x), f(x)] forms a probability distribution over the clusters 0 and cluster 1 respectively. This is the predicted probability distribution. We know for sure which cluster every point in the dataset belongs to (from y). So, we also have the true probability distribution as:\n[0, 1] when x belongs to the cluster 1 [1, 0] when x belongs to the cluster 0 A good metric to calculate the incongruity between two probability distributions is the Cross-Entropy function. As we are dealing with just 2 classes, we can use Binary Cross-Entropy (BCE). This function is available in PyTorch’s torch.nn module. If the predicted probability distribution is very similar to the true probability distribution, this function returns a small value and vice versa. We can average this value for all the data points and use it as a parameter to test how the classifier is performing.\nThis value is called the loss and mathematically, our goal now is to minimize this loss.\nTraining Now that we have defined our goal mathematically, how do we reach our goal practically? In other words, how do we find optimal values for W and B? To understand this, we will take a look at some basic calculus. Recall that we currently have random values for W and B. The process of learning or training or reaching the goal or minimizing the loss can be divided into two steps:\nForward-propagation: We feed the dataset through the classifier f(x) and use BCE to find the loss. Backpropagation: Using the loss, adjust the values of W and B to minimize the loss. The above two steps will be repeated over and over again until the loss stops decreasing. In this condition, we say that we have reached the goal!\nBackpropagation Forward propagation is simple and already discussed above. However, it is essential to take a moment to understand backpropagation as it is the key to machine learning. Recall that we have 3 parameters (variables) in W and 1 in B. So, in total, we have 4 values to optimize.\nOnce we have the loss from forward-propagation, we will calculate the gradients of the loss function with respect to each variable in the classifier. If we plot the loss for different values of each parameter, we can see that the loss is minimum at a particular value for each parameter. I have plotted the loss vs parameter for each parameter.\nAn important observation to make here is that the loss is minimized at a particular value for each of these parameters as shown by the red dot.\nLet’s consider the first plot and discuss how w1 will be optimized. The process remains the same for the other parameters. Initially, the values for W and B are chosen randomly and so (w1, loss) will be randomly placed on this curve as shown by the green dot.\nNow, the goal is to reach the red dot, starting from the green dot. In other words, we need to move downhill. Looking at the slope of the curve at the green dot, we can tell that increasing w1 (moving right) will lower the loss and therefore move the green dot closer to the red one. In mathematical terms, if the gradient of the loss with respect to w1 is negative, increase w1 to move downhill and vice versa. Therefore, w1 should be updated as:\nThe equation above is known as gradient descent equation. Here, the learning_rate controls how much we want to increase or decrease w1. If the learning_rate is large, the update will be large. This could lead to w1 going past the red dot and therefore missing the optimal value. If this value is too small, it will take forever for w1 to reach the red dot. You can try experimenting with different values of learning rate to see which works the best. In general, small values like 0.01 works well for most cases.\nIn most cases, a single update is not enough to optimize these parameters; so, the process of forward-propagation and backpropagation is repeated in a loop until the loss stops reducing further. Let’s see this in action:\nAn important observation to make is that initially the green dot moves quickly and slows down as it gradually approaches the minima. The large slope (gradient) during the first few epochs (when the green dot is far from the minima) is responsible for this large update to the parameters. The gradient decreases as the green dot approaches the minima and thus the update becomes slow. The other three parameters are trained in parallel in the exact same way. Another important observation is that the shape of the curve changes with epoch. This is due to the fact that the other three parameters (w2, w3, b) are also being updated in parallel and each parameter contributes to the shape of the loss curve.\nVisualize Let’s see how the decision boundary updates in real-time as the parameters are being updated.\nThat’s all folks! If you made it till here, hats off to you! In this article, we took a visual approach to understand how machine learning works. So far, we have seen how a simple 3D to 1D mapping, f(x), can be used to fit a decision boundary (2D plane) to a linearly separable dataset (3D). We discussed how forward propagation is used to calculate the loss followed by backpropagation where gradients of the loss with respect to parameters are calculated and the parameters are updated repeatedly in a training loop.\n","permalink":"http://localhost:43431/blog/machine-learning-visualized/","summary":"Introduction to machine learning In the traditional hard-coded approach, we program a computer to perform a certain task. We tell it exactly what to do when it receives a certain input. In mathematical terms, this is like saying that we write the f(x) such that when users feed the input x into f(x), it gives the correct output y.\nIn machine learning, however, we have a large set of inputs x and corresponding outputs y but not the function f(x).","title":"Machine Learning - Visualized"},{"content":"Introduction In this article, we will learn how PCA can be used to compress a real-life dataset. We will be working with Labelled Faces in the Wild (LFW), a large scale dataset consisting of 13233 human-face grayscale images, each having a dimension of 64x64. It means that the data for each face is 4096 dimensional (there are 64x64 = 4096 unique values to be stored for each face). We will reduce this dimension requirement, using PCA, to just a few hundred dimensions!\nPrincipal Component Analysis (PCA) Principal component analysis (PCA) is a technique for reducing the dimensionality of datasets, exploiting the fact that the images in these datasets have something in common. For instance, in a dataset consisting of face photographs, each photograph will have facial features like eyes, nose, mouth. Instead of encoding this information pixel by pixel, we could make a template of each type of these features and then just combine these templates to generate any face in the dataset. In this approach, each template will still be 64x64 = 4096 dimensional, but since we will be reusing these templates (basis functions) to generate each face in the dataset, the number of templates required will be small. PCA does exactly this. Let’s see how!\nDataset Let’s visualize some images from the dataset. You can see that each image has a complete face, and the facial features like eyes, nose, and lips are clearly visible in each image. Now that we have our dataset ready, let’s compress it.\nCompression PCA is a 4 step process. Starting with a dataset containing n dimensions (requiring n-axes to be represented):\nStep 1: Find a new set of basis functions (naxes) where some axes contribute to most of the variance in the dataset while others contribute very little. Step 2: Arrange these axes in the decreasing order of variance contribution. Step 3: Now, pick the top k axes to be used and drop the remaining n-k axes. Step 4: Now, project the dataset onto these k axes. These steps are well explained in my previous article. After these 4 steps, the dataset will be compressed from n-dimensions to just k-dimensions (k\u0026lt;n).\nStep 1 Finding a new set of basis functions (n-axes), where some axes contribute to most of the variance in the dataset while others contribute very little, is analogous to finding the templates that we will combine later to generate faces in the dataset. A total of 4096 templates, each 4096 dimensional, will be generated. Each face in the dataset can be represented as a linear combination of these templates.\nPlease note that the scalar constants (k1, k2, …, kn) will be unique for each face.\nStep 2 Now, some of these templates contribute significantly to facial reconstruction while others contribute very little. This level of contribution can be quantified as the percentage of variance that each template contributes to the dataset. So, in this step, we will arrange these templates in the decreasing order of variance contribution (most significant…least significant).\nStep 3 Now, we will keep the top k templates and drop the remaining. But, how many templates shall we keep? If we keep more templates, our reconstructed images will closely resemble the original images but we will need more storage to store the compressed data. If we keep too few templates, our reconstructed images will look very different from the original images.\nThe best solution is to fix the percentage of variance that we want to retain in the compressed dataset and use this to determine the value of k (number of templates to keep). If we do the math, we find that to retain 99% of the variance, we need only the top 577 templates. We will save these values in an array and drop the remaining templates.\nLet’s visualize some of these selected templates.\nPlease note that each of these templates looks somewhat like a face. These are called as Eigenfaces.\nStep 4 Now, we will construct a projection matrix to project the images from the original 4096 dimensions to just 577 dimensions. The projection matrix will have a shape (4096, 577), where the templates will be the columns of the matrix.\nBefore we go ahead and compress the images, let’s take a moment to understand what we really mean by compression. Recall that the faces can be generated by a linear combination of the selected templates. As each face is unique, every face in the dataset will require a different set of constants (k1, k2, …, kn) for the linear combination.\nLet’s start with an image from the dataset and compute the constants (k1, k2, …, kn), where n = 577. These constants along with the selected 577 templates can be plugged in the equation above to reconstruct the face. This means that we only need to compute and save these 577 constants for each image. Instead of doing this image by image, we can use matrices to compute these constants for each image in the dataset at the same time.\nRecall that there are 13233 images in the dataset. The matrix compressed_images contains the 577 constants for each image in the dataset. We can now say that we have compressed our images from 4096 dimensions to just 577 dimensions while retaining 99% of the information.\nCompression Ratio Let’s calculate how much we have compressed the dataset. Recall that there are 13233 images in the dataset and each image is 64x64 dimensional. So, the total number of unique values required to store the original dataset is13233 x 64 x 64 = 54,202,368 unique values.\nAfter compression, we store 577 constants for each image. So, the total number of unique values required to store the compressed dataset is13233 x 577 = 7,635,441 unique values. But, we also need to store the templates to reconstruct the images later. Therefore, we also need to store577 x 64 x 64 = 2,363,392 unique values for the templates. Therefore, the total number of unique values required to store the compressed dataset is7,635,441 + 2,363,392 = 9,998,883 unique values.\nWe can calculate the percentage compression as:\nReconstruct the Images The compressed images are just arrays of length 577 and can’t be visualized as such. We need to reconstruct it back to 4096 dimensions to view it as an array of shape (64x64). Recall that each template has a dimension of 64x64 and that each constant is a scalar value. We can use the equation below to reconstruct any face in the dataset.\nAgain, instead of doing this image by image, we can use matrices to reconstruct the whole dataset at once, with of course a loss of 1% variance.\nLet’s look at some reconstructed faces.\nWe can see that the reconstructed images have captured most of the relevant information about the faces and the unnecessary details have been ignored. This is an added advantage of data compression, it allows us to filter unnecessary details (and even noise) present in the data.\nThat’s all folks! If you made it till here, hats off to you! In this article, we learnt how PCA can be used to compress Labelled Faces in the Wild (LFW), a large scale dataset consisting of 13233 human-face images, each having a dimension of 64x64. We compressed this dataset by over 80% while retaining 99% of the information.\nColab Notebook View my Colab Notebook for a well commented code!\n","permalink":"http://localhost:43431/blog/face-dataset-compression/","summary":"Introduction In this article, we will learn how PCA can be used to compress a real-life dataset. We will be working with Labelled Faces in the Wild (LFW), a large scale dataset consisting of 13233 human-face grayscale images, each having a dimension of 64x64. It means that the data for each face is 4096 dimensional (there are 64x64 = 4096 unique values to be stored for each face). We will reduce this dimension requirement, using PCA, to just a few hundred dimensions!","title":"Face Dataset Compression using PCA"},{"content":"Introduction If you have ever taken an online course on Machine Learning, you must have come across Principal Component Analysis for dimensionality reduction, or in simple terms, for compression of data. Guess what, I had taken such courses too but I never really understood the graphical significance of PCA because all I saw was matrices and equations. It took me quite a lot of time to understand this concept from various sources. So, I decided to compile it all in one place.\nIn this article, we will take a visual (graphical) approach to understand PCA and how it can be used to compress data. Basic knowledge of Linear Algebra and Matrices is assumed. If you are new to this concept, just follow along, I have tried my best to keep this as simple as possible.\nThese days, datasets containing a large number of dimensions are increasingly common and are often difficult to interpret. One example can be a database of face photographs of let’s say, 1,000,000 people. If each face photograph has a dimension of 100x100, then the data of each face is 10000 dimensional (there are 100x100 = 10,000 unique values to be stored for each face). Now, if 1 byte is required to store the information of each pixel, then 10,000 bytes are required to store 1 face. Since there are 1000 faces in the database,10,000 x 1,000,000 = 10 GB will be needed to store the dataset.\nPrincipal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, exploiting the fact that the images in these datasets have something in common. For instance, in a dataset consisting of face photographs, each photograph will have facial features like eyes, nose, mouth. Instead of encoding this information pixel by pixel, we could make a template of each type of these features and then just combine these templates to generate any face in the dataset. In this approach, each template will still be 100x100 = 1000 dimensional, but since we will be reusing these templates (basis functions) to generate each face in the dataset, the number of templates required will be very small. PCA does exactly this.\nHow does PCA work? This part is going to be a bit technical, so bear with me! I will try to explain the working of PCA with a simple example. Let’s consider the data shown below containing 100 points each 2 dimensional (x \u0026amp; y coordinates is needed to represent each point).\nCurrently, we are using 2 values to represent each point. Let’s explain this situation in a more technical way. We are currently using 2 basis functions,x as (1, 0) and y as (0, 1). Each point in the dataset is represented as a weighted sum of these basis functions. For instance, point (2, 3) can be represented as 2(1, 0) + 3(0, 1) = (2, 3). If we omit either of these basis functions, we will not be able to represent the points in the dataset accurately. Therefore, both the dimensions necessary, and we can’t just drop one of them to reduce the storage requirement. This set of basis functions is actually the cartesian coordinate in 2 dimensions.\nIf we notice closely, we can very well see that the data approximates a line as shown by the red line below.\nNow, let’s rotate the coordinate system such that the x-axis lies along the red line. Then, the y-axis (green line) will be perpendicular to this red line. Let’s call these new x and y axes as a-axis and b-axis respectively. This is shown below.\nNow, if we use a and b as the new set basis functions (instead of using x and y) for this dataset, it wouldn’t be wrong to say that most of the variance in the dataset is along the a-axis. Now, if we drop the b-axis, we can still represent the points in the dataset very accurately, using just a-axis. Therefore, we now only need half as must storage to store the dataset and reconstruct it accurately. This is exactly how PCA works.\nPCA is a 4 step process. Starting with a dataset containing n dimensions (requiring n-axes to be represented):\nFind a new set of basis functions (naxes) where some axes contribute to most of the variance in the dataset while others contribute very little. Arrange these axes in the decreasing order of variance contribution. Now, pick the top k axes to be used and drop the remaining n-k axes. Now, project the dataset onto these k axes. After these 4 steps, the dataset will be compressed from n-dimensions to just k-dimensions (k\u0026lt;n).\nSteps For the sake of simplicity, let’s take the above dataset and apply PCA on that. The steps involved will be technical and basic knowledge of linear algebra is assumed.\nStep 1 Since this is a 2-dimensional dataset, n=2. The first step is to find the new set of basis functions (a \u0026amp; b). In the explanation above, we saw that the dataset had the maximum variance along a line and we manually chose that line as a-axis and the line the perpendicular to it as b-axis. In practice, we want this step to be automated.\nTo accomplish this, we can find the eigenvalues and eigenvectors of the covariance matrix of the dataset. Since the dataset is 2 dimensional, we will get 2 eigenvalues and their corresponding eigenvectors. Then, the 2 eigenvectors are two basis functions (new axes) and the two eigenvalues tell us the variance contribution of the corresponding eigenvectors. A large value of eigenvalue implies that the corresponding eigenvector (axis) contributes more towards the total variance of the dataset.\nStep 2 Now, sort the eigenvectors (axes) according to decreasing eigenvalues. Here, we can see that the eigenvalue for a-axis is much larger than that of theb-axis meaning that a-axis contributes more towards the dataset variance.\nThe percentage contribution of each axis towards the total dataset variance can be calculated as:\nThe above numbers prove that the a-axis contributes 99.7% towards the dataset variance and that we can drop the b-axis and lose just 0.28% of the variance.\nStep 3 Now, we will drop the b-axis and keep only the a-axis.\nStep 4 Now, reshape the first eigenvector (a-axis) into a 2x1 matrix, called the projection matrix. It will be used to project the original dataset of shape(100, 2) onto the new basis function (a-axis), thus compressing it to (100, 1).\nReconstruct the data Now, we can use the projection matrix to expand the data back to its original size, with of course a small loss of variance (0.28%).\nThe reconstructed data is shown below:\nPlease note that the variance along the b-axis (0.28%) is lost as evident by the above figure.\nThat’s all folks! If you made it till here, hats off to you! In this article, we took a graphical approach to understand how Principal Component Analysis works and how it can be used for data compression.\nColab Notebook View my Colab Notebook for a well commented code!\n","permalink":"http://localhost:43431/blog/pca-visualized/","summary":"Introduction If you have ever taken an online course on Machine Learning, you must have come across Principal Component Analysis for dimensionality reduction, or in simple terms, for compression of data. Guess what, I had taken such courses too but I never really understood the graphical significance of PCA because all I saw was matrices and equations. It took me quite a lot of time to understand this concept from various sources.","title":"Principal Component Analysis - Visualized"},{"content":"🔗 GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.txt. The NodeJS application reads the MyClipping.txt file exported by Kindle, parses it using Regex, extracts all the highlights, book names, highlight time etc and creates a JSON. It then uses Notion API to sync these highlights to a database in my Notion workspace. The app maintains a cache (JSON) containing the number of highlights synced for each book. This allows the highlights to be synced incrementally, preventing re-syncing of old highlights.\nAfter the app was received well by the open-source community and other developers contributed to improve the app, I dockerized it to make shipping the app easier. Now, the users don’t have to install any dependency. They can just use the docker run command with the path to their clippings file along with their Notion API key and database ID. This would sync their highlights to their Notion database.\nAs a part of automation, I implemented auto build and deployment of containers on push to the master branch using GitHub Actions. If a developer raises a pull request and I merge it to the master branch, the GitHub workflow automatically builds the app and deploys it to GitHub packages repository.\n","permalink":"http://localhost:43431/projects/kindle-to-notion/","summary":"🔗 GitHub Description I like reading personal improvement and mindset change type books on Kindle e-reader. Some of these books are downloaded straight from the internet and not from the Kindle store. I take highlights during my reading which I wanted to sync to my Notion workspace. There was no existing app that could do this job, so I developed my own.\nKindle exports the highlights as a file named MyClippings.","title":"Kindle to Notion"},{"content":"🔗 Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the model’s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated. Furthermore, beam search is used during inference to enhance the prediction result. The network was trained in PyTorch on an Nvidia GTX 1060 graphics card for over 80 epochs.\n","permalink":"http://localhost:43431/projects/automated-image-captioning/","summary":"🔗 Colab Notebook Description In this project, I implemented the paper Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. The neural network, a combination of CNN and LSTM, was trained on the MS COCO dataset and it learns to generate captions from images.\nAs the network generates the caption, word by word, the model’s gaze (attention) shifts across the image. This allows it to focus on those parts of the image which is more relevant for the next word to be generated.","title":"Automated Image Captioning (Bachelor Thesis)"},{"content":"🔗 View App 🔗 GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.\n","permalink":"http://localhost:43431/projects/todo-list-app/","summary":"🔗 View App 🔗 GitHub Description A to-do list web application built using React that allows the user to add, remove and edit their todos. Todo lists are stored in the browser local storage. I built this app while learning React.","title":"Todo List App"},{"content":"🔗 Colab Notebook 🔗 Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better. The neural network was trained for 30 epochs before it reached the optima.\nDuring inference, OpenCV Harr Cascades are used to detect faces in the input images. Detected faces are then cropped, resized to (224, 224), and fed to our trained neural network to predict landmarks in them. The predicted landmarks in the cropped faces are then overlayed on top of the original image.\n","permalink":"http://localhost:43431/projects/face-landmarks-detection/","summary":"🔗 Colab Notebook 🔗 Blog Post Description In this project, I trained a neural network to localize key points on faces. Resnet-18 was used as the model with some slight modifications to the input and output layer. The model was trained on the official DLib Dataset containing 6666 images along with corresponding 68-point landmarks for each face. Additionally, I wrote a custom data preprocessing pipeline in PyTorch to increase variance in the input images to help the model generalize better.","title":"Face Landmarks Detection using CNN"},{"content":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre. Various important parameters such as pupil velocity, acceleration, and fixation time were calculated for further statistical analysis. Single Shot Descriptor, with VGG16 as backbone, was used to detect the objects the user was gazing at. Additionally, a GUI was made using TkInter for ease of use.\n","permalink":"http://localhost:43431/projects/gaze-tracking-goggles/","summary":"Description The aim of the project was to build goggles which could find where the user was looking (gaze), the category of object the user was looking at, and the duration of attention on that object. The goggles had 3 camera modules, one on each eye to track the pupil movement and the third one for mapping the gaze to the real world. Thresholding was used to detect the pupils and contours were used to find its centre.","title":"Gaze-tracking Goggles"},{"content":"🔗 GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.\nRaspberry Pi runs a ROS node which communicates with another ROS node running on the host PC to transfer videos over Wi-Fi. To make the project open-source, easy to develop, and easily reproducible, the simulation environment setup has been dockerized using docker container. We are currently developing the algorithms and testing them in Gazebo Simulation.\n","permalink":"http://localhost:43431/projects/openquad/","summary":"🔗 GitHub Description The aim of the project is to build an open-source quadcopter platform for research in the field of drone autonomy. Various deep learning and computer vision algorithms will be implemented on the drone including person tracking, gesture control using human pose estimation, optical flow stabilization, obstacle avoidance, and depth estimation using monocular vision. The drone uses a Pixhawk flight controller with Raspberry Pi as a companion computer. DJI Flame Wheel-450 is used for the quadcopter frame along with some custom mountings for adding additional components.","title":"OpenQuad"},{"content":" Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\n🔗 Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris. This will assist the rescue team to focus on recovering the victims, leaving the locating task for the Robots. The unique features of the SRR above existing ATVs are active-articulation, modularity, and assisted-autonomy. Active-articulation allows the SRR to climb objects much tall than itself. Modularity allows the SRR to detach into smaller modules to enter tight spaces where the whole body can’t fit. Assisted-autonomy allows the SRR to detect the presence of objects in front and climb autonomously over them.\n","permalink":"http://localhost:43431/projects/search-and-reconnaissance-robot/","summary":"Presented in the 4th International and 19th National Conference on Machine and Mechanisms (iNaCoMM 2019)\nPublished in the Springer 2019\n🔗 Publication Description Natural disasters like earthquakes and landslides are sudden events that cause widespread destruction and major collateral damage including loss of life. Though disasters can never be prevented, their effects on mankind can surely be reduced. In this paper, we present the design and control of SRR (Search and Reconnaissance Robot), a robot capable of traversing on all terrains and locating survivors stuck under the debris.","title":"Search and Reconnaissance Robot"},{"content":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter. It uses the PID control algorithm to balance on two wheels and a simple Convolutional Neural Network is used to recognize traffic signs.\n","permalink":"http://localhost:43431/projects/sebart-pro/","summary":"Description I worked on this project single-handedly during the summer break following my freshman year at NIT- Trichy. SEBART-Pro is a robot that follows a ball while balancing on two wheels. It can also recognize traffic signs and act accordingly. It has two stepper motors for precise position control and used an Arduino Nano as the microcontroller. The robot senses the tilt using an MPU-6050 (6-axis gyroscope and accelerometer) and converts the values from these sensors into angles using a Kalman Filter.","title":"SEBART-Pro"},{"content":"Project Title: EEG-Based Deep Learning for Auditory Processing Lab: Care Computing Lab, IIT Delhi\nSupervisor: Prof. Monika Aggarwal\nAs a Technical Programmer in the Care Computing Lab at IIT Delhi, I worked on an innovative project exploring the use of electroencephalography (EEG) to decode how the brain processes auditory stimuli, focusing on the relationship between EEG signals and natural speech. The project aimed to develop deep learning models that can help detect auditory attention and provide insights for hearing loss diagnosis and next-generation smart hearing aids.\nProject Overview: Our project was part of the Auditory-EEG Challenge, where we worked on building models that can establish correlations between EEG signals and speech features. The challenge was particularly difficult due to the low signal-to-noise ratio of EEG data, requiring us to explore non-linear methods to enhance prediction accuracy. The project had two main tasks:\nTask 1: Match-Mismatch\nGiven two segments of speech and an EEG signal, we aimed to identify which segment of speech matched the brain’s response in the EEG. Task 2: Regression\nThe goal was to reconstruct the speech envelope from the EEG data, essentially decoding speech-related brain activity to create a direct mapping from the EEG to auditory stimuli. Key Responsibilities \u0026amp; Contributions: Data Preprocessing \u0026amp; Signal Representation\nImplemented advanced signal preprocessing techniques to improve the quality of EEG data, such as noise reduction and feature extraction, using techniques like Fast Fourier Transform (FFT) and Wavelet Transform. Worked on creating stimulus representations that could better relate speech signals to the EEG data for model input. Deep Learning Model Development\nDeveloped and fine-tuned deep learning models using architectures like Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs) to extract temporal and spatial features from EEG data. Implemented non-linear regression techniques to enhance the model’s ability to map EEG data to speech signals, addressing the challenges posed by the noisy EEG data. Model Evaluation and Performance Enhancement\nUsed metrics such as Mean Squared Error (MSE) and Correlation Coefficients to evaluate the model\u0026rsquo;s ability to predict speech features from EEG. Conducted rigorous cross-validation experiments to ensure that the models generalized well across different participants in the dataset. Collaborative Research \u0026amp; Insights\nCollaborated closely with researchers in the lab, as well as with faculty members, to refine our approach and interpret the results in the context of auditory processing neuroscience. Contributed to research papers and presentations, discussing the potential impact of our findings on smart hearing aids and diagnostic tools for auditory disorders. Data Handling \u0026amp; Software Development\nWorked with large datasets (containing EEG data from 85 subjects), developing robust software solutions for data management and processing. Contributed to the development of data preprocessing pipelines and model deployment scripts, which helped streamline the research process and make the analysis more efficient. This project provided valuable experience in applying deep learning and signal processing techniques to real-world healthcare challenges, particularly in the fields of auditory neuroscience and smart medical devices. ","permalink":"http://localhost:43431/experience/16bit/","summary":"Project Title: EEG-Based Deep Learning for Auditory Processing Lab: Care Computing Lab, IIT Delhi\nSupervisor: Prof. Monika Aggarwal\nAs a Technical Programmer in the Care Computing Lab at IIT Delhi, I worked on an innovative project exploring the use of electroencephalography (EEG) to decode how the brain processes auditory stimuli, focusing on the relationship between EEG signals and natural speech. The project aimed to develop deep learning models that can help detect auditory attention and provide insights for hearing loss diagnosis and next-generation smart hearing aids.","title":"Technical Programmer"},{"content":"Project Title: AI-Assisted Cardiovascular Disease Diagnostics Team Name: Soreno\nGuide: Prof. Sitikantha Roy (Department of Applied Mechanics and ScAI, IIT Delhi), Dr. Neeraj Awasthy (Director of Pediatric Cardiology, Fortis Escorts Okhla, Delhi)\nAs a GDC INDUCT Fellow at IIT Madras, I was selected to be a part of a team working on an AI-assisted diagnostic tool for cardiovascular diseases. Our project aimed to address the growing challenges within India\u0026rsquo;s healthcare system, where there is a significant burden on cardiologists in urban areas, as well as a scarcity of specialists in rural regions. The goal of this project was to create an AI-powered screening tool that not only lightens the workload for cardiologists in high-density urban hospitals but also extends access to cardiovascular diagnosis in underserved, low-resource regions.\nKey Contributions: Business Ideation and Genesis\nParticipated in structured ideation workshops at GDC to translate deep-tech research into a scalable commercial solution. Collaborated with faculty, clinicians, and industry experts to define the project’s core business vision, emphasizing AI-driven diagnostics as both a clinical support tool for overburdened doctors and a cost-effective solution for rural health workers. Drafted the initial vision document that focused on the social impact, technical innovation, and market potential, aiming to democratize access to high-quality cardiac care. Technology and Clinical Alignment\nWorked closely with Prof. Sitikantha Roy to tailor the AI model development for real-world constraints such as portability, computational efficiency, and cost-effectiveness, which are essential for scaling AI solutions in resource-limited environments. Engaged with Dr. Neeraj Awasthy to refine the tool’s clinical workflows for use in high-risk scenarios like congenital heart disease screening in children and rheumatic heart disease detection in adolescents. Developed a technology adoption roadmap to ensure smooth integration of the AI diagnostic tool into existing hospital infrastructures and mobile health platforms for wide-scale implementation. Ecosystem Study and Stakeholder Mapping\nConducted an in-depth ecosystem study of India’s healthcare system, specifically the cardiology sector, identifying key players in diagnostics and devices. Mapped the key stakeholders including primary care providers, cardiologists, procurement teams, and health-tech regulators to better understand their pain points, incentives, and opportunities for collaboration. Primary Market Research and Need Assessment\nConducted interviews with over 26 leading cardiologists and healthcare administrators across major Indian cities (Delhi, Mumbai, Chennai, Bangalore, and Hyderabad) to assess the real-world clinical needs and challenges. Key findings included: In tier-1 cities, cardiologists expressed a strong need for AI tools to assist with triage, risk stratification, and diagnostic decision-making due to overwhelming patient volumes. In tier-2/3 towns and rural areas, a shortage of specialists led to a demand for AI-powered screening tools that could be operated by general physicians or health workers. Identified challenges such as the lack of interoperability with hospital IT systems, skepticism towards AI’s accuracy, and regulatory hurdles in achieving clinical validation. Business Strategy and Commercial Roadmap\nDeveloped a business plan under the mentorship of Mr. Ganesan V.R., focusing on customer segmentation, go-to-market strategy, and revenue models. Created a plan for pilot implementations in CSR-funded rural healthcare programs and hospitals looking to innovate their cardiac care offerings. Proposed a device licensing model, complemented by SaaS-based analytics dashboards and long-term data monetization through hospital and insurance partnerships. Customer Ecosystem and Product Adoption Strategy\nMapped out the customer journey, outlining decision-makers such as department heads, administrators, and clinical technology officers. Simulated the adoption process, identifying barriers to integration and ensuring smooth transitions from prototype to full-scale deployment. Market Opportunity and Competitive Analysis\nConducted a competitive analysis of AI-powered cardiovascular diagnostic tools, benchmarking Soreno against existing solutions in both Indian startups and global companies. Identified a gap in the pediatric cardiology sector, with limited players targeting low-cost, scalable diagnostic solutions for rural and semi-urban areas. Created market forecasts based on the Total Addressable Market (TAM) and Serviceable Available Market (SAM) for AI-driven diagnostic tools, projecting patient reach and revenue over the next 5 years. Outcome: Successfully developed an AI-assisted screening tool for cardiovascular diseases, focusing on both urban hospitals and rural areas. Gained invaluable exposure to the integration of AI in healthcare and learned how to align business and clinical goals to create scalable, impactful solutions. Provided a roadmap for future scalability and adoption, aiming to empower healthcare professionals and expand access to high-quality cardiac care in underserved regions. ","permalink":"http://localhost:43431/experience/buyerassist/","summary":"Project Title: AI-Assisted Cardiovascular Disease Diagnostics Team Name: Soreno\nGuide: Prof. Sitikantha Roy (Department of Applied Mechanics and ScAI, IIT Delhi), Dr. Neeraj Awasthy (Director of Pediatric Cardiology, Fortis Escorts Okhla, Delhi)\nAs a GDC INDUCT Fellow at IIT Madras, I was selected to be a part of a team working on an AI-assisted diagnostic tool for cardiovascular diseases. Our project aimed to address the growing challenges within India\u0026rsquo;s healthcare system, where there is a significant burden on cardiologists in urban areas, as well as a scarcity of specialists in rural regions.","title":"GDC INDUCT Fellow, IIT Madras"},{"content":"🩺 Multimodal Deep Learning for Chest X-Ray–Based Cardiology Diagnosis At Augmented Medicine Pvt. Ltd., I worked as an AI Software Engineer in a core research and development role aimed at integrating artificial intelligence into radiological workflows to assist in the early detection and triaging of cardiovascular diseases using chest X-rays.\n🔍 Project Focus Designed and implemented a deep learning pipeline for diagnosing heart-related diseases using chest X-ray (CXR) images as the primary input modality Integrated multimodal inputs including patient metadata (age, gender, medical history) to enhance model performance and contextual relevance The primary goal was to create a high-specificity model that minimizes false positives, allowing radiologists to confidently rule out normal (negative) cases and focus their attention on complex or abnormal cases 💡 Key Technical Highlights Utilized pre-trained CNN architectures (e.g., DenseNet121, Inceptionv1 and fine-tuned them on large-scale public datasets like NIH ChestX-ray14 and CheXpert Developed custom loss functions and specificity-prioritized training strategies to improve true-negative prediction rates, aligning with clinical needs for safe automation Incorporated Grad-CAM and saliency maps to provide explainability, highlighting pathological features that led to the model’s decision Employed class imbalance handling techniques (e.g., focal loss, weighted sampling) to address the natural skew in CXR datasets toward negative cases Deployed model evaluation frameworks with metrics such as specificity, ROC-AUC, and precision-recall curves, tailored to cardiology diagnostic thresholds 💼 Clinical Relevance \u0026amp; Outcome Reduced radiologist workload by enabling AI-driven triage, automatically filtering normal cases for expedited review Provided decision support for early diagnosis of conditions such as cardiomegaly, pulmonary edema, and congestive heart failure Demonstrated potential for integration into Picture Archiving and Communication Systems (PACS) for real-time clinical application ","permalink":"http://localhost:43431/experience/tumunich/","summary":"🩺 Multimodal Deep Learning for Chest X-Ray–Based Cardiology Diagnosis At Augmented Medicine Pvt. Ltd., I worked as an AI Software Engineer in a core research and development role aimed at integrating artificial intelligence into radiological workflows to assist in the early detection and triaging of cardiovascular diseases using chest X-rays.\n🔍 Project Focus Designed and implemented a deep learning pipeline for diagnosing heart-related diseases using chest X-ray (CXR) images as the primary input modality Integrated multimodal inputs including patient metadata (age, gender, medical history) to enhance model performance and contextual relevance The primary goal was to create a high-specificity model that minimizes false positives, allowing radiologists to confidently rule out normal (negative) cases and focus their attention on complex or abnormal cases 💡 Key Technical Highlights Utilized pre-trained CNN architectures (e.","title":"AI Software Engineer"},{"content":"Project Title: Advancing Medical Imaging with Deep Learning: Ultrasound \u0026amp; Generative Models Lab: Care Computing Lab, IIT Delhi\nSupervisor: Prof. Monika Aggarwal\nAs a PhD Researcher in the Care Computing Lab at IIT Delhi, I have focused on the integration of cutting-edge deep learning techniques with medical imaging, specifically ultrasound imaging. My research aims to push the boundaries of medical diagnostics by developing advanced machine learning models, including the use of CLIP-based models, diffusion models, and large language models (LLMs), to improve the accuracy and efficiency of ultrasound-based diagnostics in healthcare.\nResearch Overview: The primary objective of my research is to apply state-of-the-art AI technologies to enhance the interpretation of ultrasound images, which are widely used in clinical practice for diagnostics. Traditional ultrasound image interpretation requires expert knowledge, and our work seeks to improve both the automated image analysis and predictive capabilities for better decision-making. The integration of advanced deep learning models has the potential to enable real-time, precise diagnostic tools that can support clinicians, particularly in low-resource settings.\nKey Areas of Focus: Ultrasound Image Analysis with Deep Learning:\nDeveloped and fine-tuned deep neural network models (CNNs, U-Nets) to analyze ultrasound images for various applications, such as detecting anomalies, organ segmentation, and improving image resolution. Implemented multi-modal learning techniques, combining ultrasound images with patient history and clinical data to improve diagnostic predictions and risk stratification. Generative Models: Diffusion Models \u0026amp; CLIP:\nApplied diffusion models for data augmentation in medical imaging, allowing the generation of synthetic ultrasound images that mimic real-world data, thus enhancing training datasets for deep learning models. Integrated CLIP (Contrastive Language-Image Pretraining) models to bridge the gap between visual and textual information, enabling ultrasound images to be analyzed in context with clinical notes, improving diagnostic interpretation and report generation. Utilized generative models to synthesize ultrasound images with higher resolution and fidelity, allowing clinicians to examine even the subtlest of diagnostic features that would be missed in lower-quality images. Interdisciplinary Collaboration \u0026amp; Mentorship:\nCollaborated closely with experts in medical imaging, radiology, and healthcare technology to ensure the models met both technical and clinical needs. Mentored master\u0026rsquo;s and undergraduate students in the lab, helping them navigate the complexities of deep learning in healthcare applications. Impact and Future Directions: The integration of deep learning with ultrasound imaging and generative models represents a significant advancement in medical technology. My work aims to create models that not only improve diagnostic performance but also democratize healthcare by enabling high-quality medical imaging in resource-constrained environments. In the future, I plan to explore the use of these models in more specialized fields such as cardiology, oncology, and maternal health.\nThis research is positioning itself at the intersection of AI, medical imaging, and healthcare accessibility, driving innovations that can transform how ultrasound technology is used in clinical practice.\n","permalink":"http://localhost:43431/experience/clio/","summary":"Project Title: Advancing Medical Imaging with Deep Learning: Ultrasound \u0026amp; Generative Models Lab: Care Computing Lab, IIT Delhi\nSupervisor: Prof. Monika Aggarwal\nAs a PhD Researcher in the Care Computing Lab at IIT Delhi, I have focused on the integration of cutting-edge deep learning techniques with medical imaging, specifically ultrasound imaging. My research aims to push the boundaries of medical diagnostics by developing advanced machine learning models, including the use of CLIP-based models, diffusion models, and large language models (LLMs), to improve the accuracy and efficiency of ultrasound-based diagnostics in healthcare.","title":"PhD Researcher"},{"content":"🧠 Machine Learning for Biomedical Disease Prediction Guide: Prof. Subramani Kanagaraj (Biomedical Device Lab, IIT Guwahati)\nAs part of a semester-long research initiative under the guidance of Prof. Subramani Kanagaraj, I worked on developing machine learning models aimed at improving disease prediction accuracy using biomedical datasets. The project spanned various stages of the machine learning lifecycle — from data preprocessing and feature engineering to model development, validation, and real-time implementation.\n🔬 Key Contributions Heart Disease Prediction using Logistic Regression\nBuilt a predictive model leveraging logistic regression to estimate the probability of heart disease based on multiple clinical parameters (age, cholesterol level, blood pressure, etc.). The model was trained and tested on real-world datasets, and later integrated into a prototype for real-time inference, demonstrating potential as a decision-support tool in clinical settings.\nBreast Cancer Classification using Neural Networks\nDeveloped a feedforward neural network to classify malignant and benign breast tumors using the Wisconsin Diagnostic Breast Cancer dataset. Applied techniques like dropout, batch normalization, and learning rate scheduling to optimize model performance, achieving over 95% accuracy on test data.\nBrain MRI Image Segmentation using U-Net\nInitiated research on applying U-Net, a convolutional neural network architecture, for semantic segmentation of brain MRI images. The objective was to identify tumor regions and anomalies, a crucial step in automated diagnosis of neurological conditions. Preprocessing involved skull stripping, normalization, and augmentation techniques to handle limited dataset availability.\n📈 Outcome and Impact Successfully demonstrated that classical and deep learning models can be effectively used in biomedical diagnostics Designed modular ML pipelines that can be adapted for additional diseases or imaging modalities Gained hands-on experience with tools and libraries including Scikit-learn, TensorFlow, Keras, and OpenCV Laid the groundwork for potential future research into explainable AI in medical imaging ","permalink":"http://localhost:43431/experience/origin-health/","summary":"🧠 Machine Learning for Biomedical Disease Prediction Guide: Prof. Subramani Kanagaraj (Biomedical Device Lab, IIT Guwahati)\nAs part of a semester-long research initiative under the guidance of Prof. Subramani Kanagaraj, I worked on developing machine learning models aimed at improving disease prediction accuracy using biomedical datasets. The project spanned various stages of the machine learning lifecycle — from data preprocessing and feature engineering to model development, validation, and real-time implementation.","title":"Semester Project"},{"content":"🔗 Medium Profile Description Authored and published over 30 technical and thought-leadership articles on Medium, focusing on AI, machine learning, and emerging technologies Simplified complex technical concepts for a broad audience, helping increase article reach and engagement across platforms Collaborated with other writers and editors to maintain publication quality and SEO best practices Accumulated 50,000+ reads and 5,000+ followers through consistent, high-quality writing and topic relevance ","permalink":"http://localhost:43431/experience/iit-madras/","summary":"🔗 Medium Profile Description Authored and published over 30 technical and thought-leadership articles on Medium, focusing on AI, machine learning, and emerging technologies Simplified complex technical concepts for a broad audience, helping increase article reach and engagement across platforms Collaborated with other writers and editors to maintain publication quality and SEO best practices Accumulated 50,000+ reads and 5,000+ followers through consistent, high-quality writing and topic relevance ","title":"Content Writer"},{"content":"","permalink":"http://localhost:43431/projects/obsidian-publish-github-action/","summary":"","title":""}]